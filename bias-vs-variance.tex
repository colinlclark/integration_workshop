\section{Bias and Variance for Learning in 1-D}
In this project, you will compare the difference between \textit{approximation} (finding the best possible candidate to model a function) and \textit{learning} (finding the best possible candidate given limited information). 
You will also analyze the extra error incurred by using limited information, and explore one possible way of reducing this error.\\

\noindent\textit{Approximation:}\\
Let $f(x) = x^2(1-x)$ where $0 \leq x \leq 1$. We wish to find the `best' linear model (or approximation) for $f$. That is, for functions in the form $g(x; \alpha, \beta) =  \alpha +  \beta x$,  we wish to find the parameters $ \alpha$ and $ \beta$ that minimize the error we would expect to incur if we used the function $g$ to model (or approximate) the function $f$. 
\begin{center}
\begin{tikzpicture}[xscale=5,yscale=10]
\draw[thick,domain=0:1,variable=\x,orange!90!black] plot ({\x},{.02 + .1*\x}) node[above right] {$y = g(x; \alpha, \beta)$};
\draw[thick,domain=0:1,variable=\x,blue] plot ({\x},{\x*\x*(1-\x)}) node[above right] {$y = f(x)$};
  \draw[->] (-.11, 0) -- (1.1, 0) node[right] {$x$};
  \draw[->] (0, -.02) -- (0, .2) node[above] {$y$};
\end{tikzpicture}
\end{center}
For this problem, we will define the error as
\begin{equation*}
\text{Error}(x;\alpha,\beta) = \Big(f(x) - g(x; \alpha, \beta)\Big)^2 
\quad \text{and} \quad
\text{TotalError}(\alpha,\beta) = \int_0^1 \text{Error}(x; \alpha,\beta) dx.
% \text{Error}(f,g) = \int_0^1 \bigg(f(x) - g(x; \alpha, \beta)\bigg)^2 dx.
\end{equation*} 
Use pencil-and-paper (or Mathematica, WolframAlpha, etc) to find a simplified expression for the error between $f$ and $g$ as a function of the parameters $ \alpha$ and $ \beta$. Find the values of $ \alpha$ and $ \beta$ that minimize the total error and find the corresponding total error.\\

\noindent\textit{Learning:}\\
In many practical applications, we only have imperfect knowledge of $f$, and therefore it is impossible to know the \textit{best possible} parameters $\alpha, \beta$. 
Often, we must commit to a choice of sub-optimal parameters that are chosen after only a small glimpse of $f$. 
In this project, you will estimate and explore the extra error incurred when using an imperfect choice of parameters.\\
\noindent\textit{Terminology:} For each value of $x$, the \textit{bias} of a model is defined as the the difference between $\bar{g}(x;\alpha,\beta)$, the expected (or average) value of $g(x;\alpha,\beta)$, and the true value of $f$.
\begin{equation}
\text{Bias}(x) = \bar{g}(x;\alpha,\beta) - f(x)
\end{equation}
\noindent\textit{Terminology:} For each value of $x$, the \textit{variance} of a model is defined as the expected (or average) value of the square of the difference between the value of $g(s;\alpha,\beta)$ and the expected value of $g(x;\alpha,\beta)$.
\begin{equation}
\text{Variance}(x) = \bigg\langle \big( g(x;\alpha,\beta) - \bar{g}(x;\alpha,\beta)\big)^2 \bigg\rangle
\end{equation}
Implement the following process to estimate how well a linear function that is parameterized by only two random data points can be used to model $f$.
\begin{enumerate}\setlength{\itemsep}{0pt}
  \item Create the variable \texttt{xx} by discretizing the interval $0 \leq x \leq 1$ into 101 points. That is, set \texttt{xx}$ = (0,\, 0.01,\, 0.02,\, \dots,\, 1)$.  Plot $f(x)$ in blue. 
    \item Generate two random data points on $x_1, x_2 \in [0,1]$. 
    \item Evaluate $y_1 =f(x_1)$ and $y_2 = f(x_2)$.  Plot $(x_1,y_1)$ and $(x_2,y_2)$ as two orange dots on the same figure as $f$. 
    \item Using \textbf{only} $(x_1,y_1)$ and $(x_2,y_2)$, compute a `best guess' for parameters $\alpha$ and $\beta$.
    \item Use your best guess parameters $\alpha$ and $\beta$ to evaluate the linear model $g(x;\alpha, \beta)$ for each $x \in \texttt{xx}$ and record the results. Plot $g(x;\alpha,\beta)$ in orange on the same figure.
\end{enumerate}
Repeat steps 1-5 several times and save their plots.\\ 

Repeat steps 1-5 1000 times (without plotting) and record the results in a $101 \times 1000$ array.
For each $x$, compute $\bar{g}(x)$, the average value predicted by the linear model $g(x;\alpha,\beta)$ and plot it on the same figure as a plot of $f(x)$. 
On the same figure, plot $g_{5}(x)$, $g_{25}(x)$, $g_{75}(x)$ and $g_{95}(x)$, the 5\textsuperscript{th}, 25\textsuperscript{th}, 75\textsuperscript{th} and 95\textsuperscript{th} quantiles of the values predicted by the linear model.  \\

\noindent\textit{Error Reduction for Learning:}\\
Someone proposes that the variance is so big that it makes the linear model unusable.  
They suggest that we should approximate $f$ by a constant function  $h(x;\alpha)$. 
Repeat the steps above to estimate the bias and the variance of the constant model. \\


